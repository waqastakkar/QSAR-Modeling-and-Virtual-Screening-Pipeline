# QSAR-Modeling-and-Virtual-Screening-Pipeline
A full QSAR and virtual screening framework including data preparation, SMILES-to-SDF conversion, RDKit fingerprints, SHAP-based explainable models, and database screening. Designed for reproducible computational chemistry and drug discovery workflows.
This repository presents a complete cheminformatics and machine learning workflow developed by Muhammad Waqas (2025) for preparing molecular datasets, generating structural fingerprints, training QSAR models, interpreting model behavior with SHAP, and performing virtual screening on large compound collections. The workflow is organized into modular scripts so that each step can be executed independently or integrated into an end-to-end pipeline.
The process begins with Data_cleaning.py, which prepares bioactivity datasets by removing missing or invalid assay values, checking concentration units when necessary, converting IC₅₀ values from nanomolar to pIC₅₀, generating binary activity labels when required, and handling SMILES canonicalization and duplicates. This ensures that the dataset entering the modeling phase is consistent and ready for fingerprint generation.
Once the data table is prepared, smiles_to_sdf.py converts SMILES structures to an SDF file while retaining all metadata. When conformer generation is enabled, the script builds 3D geometries using ETKDG and optimizes them with MMFF or UFF, providing structures suitable for downstream fingerprinting and structural analysis.
For molecular representation, the workflow includes two options. generate_fingerprints.py computes fingerprints directly from an SDF file and supports Morgan, MACCS, RDKit, AtomPair, and Torsion fingerprints, with automatic fallback between recent and legacy RDKit APIs. For users working with folders of SMILES spreadsheets, batch_fingerprinting.py processes multiple files, cleans SMILES with optional salt removal, handles fingerprint errors, and merges all processed outputs into a unified matrix. These options allow flexible handling of diverse dataset formats.
To refine the fingerprint matrix before modeling, filter_fingerprints.py removes non-informative features. It filters constant or low-variance bits, prunes rare bits based on a defined threshold, and applies correlation-based reduction to eliminate redundant features. Metadata columns are preserved throughout, ensuring that structural identifiers remain linked to the curated feature space.
The core modeling script, qsar_modeling_shap.py, trains multiple machine-learning models in parallel, selects top performers, tunes hyperparameters, and produces detailed reports. Supported models include Logistic Regression, Random Forests, SVM, Extra Trees, XGBoost, LightGBM, and optional neural networks using Keras or PyTorch. After training, the script computes SHAP values for each model type, generates global and local interpretability outputs, and records feature importance metrics, enabling users to understand the molecular features driving predictions.
After model development, screen_database.py performs virtual screening on external compound libraries. The script aligns fingerprint columns to the model’s expected features, handles mismatches in fingerprint length or naming, and applies controlled fallback rules. It produces prediction files, ranked subsets, score histograms, and a screening manifest that records all parameters used, ensuring transparency and reproducibility.
Together, these scripts form a coherent and reproducible QSAR pipeline that guides users from raw chemical data to predictive modeling and large-scale screening. The repository supports transparent reporting, modern interpretability standards through SHAP, and integration with diverse molecular datasets. All scripts are authored and copyrighted by Muhammad Waqas (2025) and are intended for researchers seeking a rigorous and user-oriented cheminformatics solution.

